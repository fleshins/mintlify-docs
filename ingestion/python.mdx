---
title: "Python Quickstart — Send Agent Traces to Brixo"
description: "Python Quickstart — Send Agent Traces to Brixo"
---

**Goal:** In under 10 minutes, send rich agent traces from your Python app to **Brixo** using **both** OpenLLMetry (Traceloop) and OpenInference.

---

## TL;DR

1. Install SDKs + OpenTelemetry
2. Configure the Brixo OTLP endpoint and API key
3. Initialize OpenInference and Traceloop (OpenLLMetry) instrumentation
4. Run your app — traces flow to Brixo

---

## Prerequisites

- Python 3.9+
- Brixo account + API key
- Internet access from your app to Brixo ingest (OTLP/HTTP)

Create a `.env` file or export environment variables:

```bash
export OTEL_EXPORTER_OTLP_TRACES_HEADERS="Brixo-Auth=Bearer <BRIXO_TOKEN>"
export OTEL_EXPORTER_OTLP_ENDPOINT="https://otel.brixo.com:4318"
```

Brixo accepts OTLP over HTTP. Authentication is via the `Brixo-Auth` header.

---

## Step 1 — Install dependencies

```bash
# Core OpenTelemetry
pip install opentelemetry-sdk opentelemetry-exporter-otlp

# OpenLLMetry (Traceloop)
pip install traceloop-sdk

# OpenInference instrumentations
pip install openinference-instrumentation-openai openinference-instrumentation-langchain
```

---

## Step 2 — Create OTLP exporter to export traces to Brixo

```python
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
exporter = OTLPSpanExporter()
```

The export will be created using the defined environment variables values.

---

## Step 3 — Initialize instrumentation packages(OpenLLMetry and OpenInference)

```python
from openinference.instrumentation.langchain import LangChainInstrumentor
from traceloop.sdk import Traceloop

LangChainInstrumentor().instrument()
Traceloop.init(app_name='brixo-demo', exporter=exporter)
```

OpenInference and OpenLLMetry automatically instruments common LLM frameworks and enriches spans with agent semantics. It is necessary to keep the same initialization sequence.

---

## Step 5 — Run the app inside a new trace span

After the configuration, start a new trace span for the agent run:

```python
from my_lang_graph_agent import agent
from opentelemetry import trace

tracer = trace.get_tracer(__name__)

with tracer.start_as_current_span('Agent'):
	messages = [HumanMessage(content="Add 3 and 4.")]
	response = agent.invoke({"messages": messages})
```

---

## Step 5.1 — Full example (single file)

```python
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
from openinference.instrumentation.langchain import LangChainInstrumentor
from traceloop.sdk import Traceloop

exporter = OTLPSpanExporter()
LangChainInstrumentor().instrument()
Traceloop.init(app_name='brixo-demo', exporter=exporter)

from my_lang_graph_agent import agent
from opentelemetry import trace

tracer = trace.get_tracer(__name__)
with tracer.start_as_current_span('Agent'):
	messages = [HumanMessage(content="Add 3 and 4.")]
	response = agent.invoke({"messages": messages})
```

That’s it — your Python app is now sending rich, merged agent traces to **Brixo**.

---

## Sampling and PII

- **Sampling:** Use `ParentBased(TraceIdRatioBased(0.2))` to manage span volume.
- **PII:** Redact sensitive data before export. Both OpenLLMetry and OpenInference support custom attribute filtering.

---

## Troubleshooting

**No traces:**

- Check `BRIXO_API_KEY` and endpoint
- Ensure telemetry is initialized before other imports

---

## FAQ

### Why both OpenLLMetry and OpenInference?

They capture different layers of context — OpenLLMetry focuses on LLM/agent orchestration, OpenInference on tool and framework spans. Brixo merges and normalizes both for full insight.

### Do I need a Collector?

No. Brixo supports direct OTLP/HTTP export.

### Which frameworks are supported?

Anything OpenLLMetry or OpenInference supports: OpenAI, Anthropic, LangChain, LlamaIndex, CrewAI, AutoGen, Bedrock, etc.

---